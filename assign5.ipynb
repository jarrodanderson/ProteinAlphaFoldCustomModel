{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"assign5.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP8BwMoUfPJ6sQ6ogw+7LTT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"1HSCmt36ld4L","colab_type":"code","colab":{}},"source":["training_file = \"drive/My Drive/assign4data/training_30\"\n","validation_file = \"drive/My Drive/assign4data/validation\"\n","testing_file = \"drive/My Drive/assign4data/testing\"\n","ng_val = 1\n","\n","INVALID_ANGLE = 10\n","import glob\n","import os.path\n","import os\n","import platform\n","import numpy as np\n","import h5py\n","import torch\n","import torch.nn as nn\n","import torch.utils.model_zoo as model_zoo\n","from torch.nn.functional import softmax \n","import pickle as pkl\n","import random\n","import torch.optim as optim\n","import subprocess\n","import torch.utils.data\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kI9I6yBhliQJ","colab_type":"code","outputId":"fd2d0735-3d86-4c6b-8579-6e653fde672a","executionInfo":{"status":"ok","timestamp":1588525011855,"user_tz":240,"elapsed":21307,"user":{"displayName":"Rahul Puppala","photoUrl":"https://lh6.googleusercontent.com/-a3E4o0T7KT8/AAAAAAAAAAI/AAAAAAAAAq4/SoPeLsACGtc/s64/photo.jpg","userId":"12827901970476877756"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["#skip this section if not running on colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UghHtBYKLbH-","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"9XjELf198apB","colab_type":"code","colab":{}},"source":["\n","\n","\n","import numpy as np\n","from Bio.PDB.vectors import Vector, calc_angle, calc_dihedral\n","\n","\n","def process_tertiary(tertiary):\n","    '''compute the bond lengths, bond angles, and dihedral angles'''\n","    phi = []\n","    psi = []\n","    omega = []\n","    bond_angle_CNCa = []\n","    bond_angle_NCaC = []\n","    bond_angle_CaCN = []\n","    bond_len_NCa = []\n","    bond_len_CaC = []\n","    bond_len_CN = []\n","    # convert tertiary coords into Vectors\n","    pV = [vec for vec in map(lambda v: Vector(v[0], v[1], v[2]),\n","                             zip(tertiary[0], tertiary[1], tertiary[2]))]\n","\n","    for i in range(0, len(pV), 3):\n","        # check for zero coords\n","        norm_im1 = False\n","        norm_i = False\n","        norm_i1 = False\n","        norm_i2 = False\n","        norm_i3 = False\n","        norm_i4 = False\n","        if i > 0 and pV[i-1].norm() > 0:\n","            norm_im1 = True\n","        if pV[i].norm() > 0:\n","            norm_i = True\n","        if pV[i+1].norm() > 0:\n","            norm_i1 = True\n","        if pV[i+2].norm() > 0:\n","            norm_i2 = True\n","        if i + 3 < len(pV) and pV[i+3].norm() > 0:\n","            norm_i3 = True\n","        if i + 3 < len(pV) and pV[i+4].norm() > 0:\n","            norm_i4 = True\n","\n","        # compute bond lengths\n","        if norm_im1 and norm_i:\n","            blen_CN = (pV[i-1]-pV[i]).norm()\n","            bond_len_CN.append(blen_CN)\n","\n","        if norm_i and norm_i1:\n","            blen_NCa = (pV[i]-pV[i+1]).norm()\n","            bond_len_NCa.append(blen_NCa)\n","\n","        if norm_i1 and norm_i2:\n","            blen_CaC = (pV[i+1]-pV[i+2]).norm()\n","            bond_len_CaC.append(blen_CaC)\n","\n","        # compute bond angles\n","        if norm_im1 and norm_i and norm_i1:\n","            theta_CNCa = calc_angle(pV[i-1], pV[i], pV[i+1])  # C-N-Ca\n","            bond_angle_CNCa.append(theta_CNCa)\n","\n","        if norm_i and norm_i1 and norm_i2:\n","            theta_NCaC = calc_angle(pV[i], pV[i+1], pV[i+2])  # N-Ca-C\n","            bond_angle_NCaC.append(theta_NCaC)\n","\n","        if norm_i1 and norm_i2 and norm_i3:\n","            theta_CaCN = calc_angle(pV[i+1], pV[i+2], pV[i+3])  # Ca-C-N\n","            bond_angle_CaCN.append(theta_CaCN)\n","\n","        # compute dihedral angles\n","        if norm_im1 and norm_i and norm_i1 and norm_i2:\n","            phi_i = calc_dihedral(\n","                pV[i-1], pV[i], pV[i+1], pV[i+2])  # N-Ca-C-N\n","        else:\n","            phi_i = INVALID_ANGLE\n","        phi.append(phi_i)\n","\n","        if norm_i and norm_i1 and norm_i2 and norm_i3:\n","            psi_i = calc_dihedral(\n","                pV[i], pV[i+1], pV[i+2], pV[i+3])  # C-N-Ca-C\n","        else:\n","            psi_i = INVALID_ANGLE\n","        psi.append(psi_i)\n","\n","        if norm_i1 and norm_i2 and norm_i3 and norm_i4:\n","            omega_i = calc_dihedral(\n","                pV[i+1], pV[i+2], pV[i+3], pV[i+4])  # Ca-C-N-Ca\n","        else:\n","            omega_i = INVALID_ANGLE\n","        omega.append(omega_i)\n","\n","    return (phi, psi, omega, bond_angle_NCaC, bond_angle_CaCN,\n","            bond_angle_CNCa, bond_len_CN, bond_len_NCa, bond_len_CaC)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lmr4ulp-okMA","colab_type":"code","colab":{}},"source":["#functions from util\n","AA_ID_DICT = {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9,\n","              'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17,\n","              'V': 18, 'W': 19, 'Y': 20}\n","MAX_SEQUENCE_LENGTH = 100\n","def encode_primary_string(primary):\n","    return list([AA_ID_DICT[aa] for aa in primary])\n","\n","def calc_pairwise_distances(chain_a, chain_b, use_gpu):\n","    distance_matrix = torch.Tensor(chain_a.size()[0], chain_b.size()[0]).type(torch.float)\n","    # add small epsilon to avoid boundary issues\n","    epsilon = 10 ** (-4) * torch.ones(chain_a.size(0), chain_b.size(0))\n","    if use_gpu:\n","        distance_matrix = distance_matrix.cuda()\n","        epsilon = epsilon.cuda()\n","\n","    for idx, row in enumerate(chain_a.split(1)):\n","        distance_matrix[idx] = torch.sum((row.expand_as(chain_b) - chain_b) ** 2, 1).view(1, -1)\n","\n","    return torch.sqrt(distance_matrix + epsilon)\n","\n","def read_protein_from_file(file_pointer):\n","    \"\"\"The algorithm Defining Secondary Structure of Proteins (DSSP) uses information on e.g. the\n","    position of atoms and the hydrogen bonds of the molecule to determine the secondary structure\n","    (helices, sheets...).\n","    \"\"\"\n","    dict_ = {}\n","    _dssp_dict = {'L': 0, 'H': 1, 'B': 2, 'E': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n","    _mask_dict = {'-': 0, '+': 1}\n","\n","    while True:\n","        next_line = file_pointer.readline()\n","        if next_line == '[ID]\\n':\n","            id_ = file_pointer.readline()[:-1]\n","            dict_.update({'id': id_})\n","        elif next_line == '[PRIMARY]\\n':\n","            primary = encode_primary_string(file_pointer.readline()[:-1])\n","            dict_.update({'primary': primary})\n","        elif next_line == '[EVOLUTIONARY]\\n':\n","            evolutionary = []\n","            for _residue in range(21):\n","                evolutionary.append([float(step) for step in file_pointer.readline().split()])\n","            dict_.update({'evolutionary': evolutionary})\n","        elif next_line == '[SECONDARY]\\n':\n","            secondary = list([_dssp_dict[dssp] for dssp in file_pointer.readline()[:-1]])\n","            dict_.update({'secondary': secondary})\n","        elif next_line == \"[TERTIARY]\\n\":\n","            tertiary = []\n","            # 3 dimension\n","            for _axis in range(3):\n","                next_line = file_pointer.readline()\n","                tertiary.append(\n","                    [float(coord)/100 for coord in next_line.split()])\n","            phi, psi, omega,\\\n","                    bond_angle_NCaC, bond_angle_CaCN, bond_angle_CNCa,\\\n","                    bond_len_CN, bond_len_NCa, bond_len_CaC = process_tertiary(\n","                        tertiary)\n","            dict_.update({\"tertiary\": tertiary})\n","            dict_.update({\"phi\": phi})\n","            dict_.update({\"psi\": psi})\n","\n","            \n","        elif next_line == '[MASK]\\n':\n","            mask = list([_mask_dict[aa] for aa in file_pointer.readline()[:-1]])\n","            dict_.update({'mask': mask})\n","        elif next_line == '\\n':\n","            return dict_\n","        elif next_line == '':\n","            return None\n","\n","\n","def process_file(input_file, use_gpu):\n","\n","    input_file_pointer = open(input_file, \"r\")\n","    all_proteins = []\n","    while True:\n","        # while there's more proteins to process\n","        next_protein = read_protein_from_file(input_file_pointer)\n","        if next_protein is None:\n","            break\n","\n","        sequence_length = len(next_protein['primary'])\n","\n","        if sequence_length > MAX_SEQUENCE_LENGTH:\n","            #print(\"Dropping protein as length too long:\", sequence_length)\n","            continue\n","\n","        phi = next_protein['phi']\n","        psi = next_protein['psi']\n","\n","        primary_padded = np.zeros(MAX_SEQUENCE_LENGTH)\n","        tertiary_padded = np.zeros((3, MAX_SEQUENCE_LENGTH))\n","        evo_padded = np.zeros((21,MAX_SEQUENCE_LENGTH))\n","        phi_padded = np.zeros(MAX_SEQUENCE_LENGTH)\n","        psi_padded = np.zeros(MAX_SEQUENCE_LENGTH)\n","        \n","        # masking and padding here happens so that the stored dataset is of the same size.\n","        # when the data is loaded in this padding is removed again.\n","        primary_padded[:sequence_length] = next_protein['primary']\n","        phi_padded[:sequence_length] = phi\n","        psi_padded[:sequence_length] = psi\n","        phi_mask = torch.Tensor([1 if x!=INVALID_ANGLE else 0 for x in phi_padded])\n","        psi_mask = torch.Tensor([1 if x!=INVALID_ANGLE else 0 for x in psi_padded])\n","        mask_padded = np.zeros(MAX_SEQUENCE_LENGTH)\n","        mask_padded[:sequence_length] = next_protein['mask']\n","        mask_padded = mask_padded*phi_mask.numpy()*psi_mask.numpy()\n","\n","        next_protein['tertiary'] = [next_protein['tertiary'][i] for i in range(1, len(next_protein['tertiary']), 3)]\n","        t_transposed = np.ravel(np.array(next_protein['tertiary']).T)\n","        t_reshaped = np.reshape(t_transposed, (sequence_length, 3)).T\n","\n","        e_transposed = np.ravel(np.array(next_protein['evolutionary']).T)\n","        e_reshaped = np.reshape(e_transposed, (sequence_length,21)).T\n","\n","        tertiary_padded[:, :sequence_length] = t_reshaped\n","        evo_padded[:,:sequence_length] = e_reshaped\n","\n","\n","        mask = torch.Tensor(mask_padded).type(dtype=torch.bool)\n","\n","        prim = torch.masked_select(torch.Tensor(primary_padded)\n","                                   .type(dtype=torch.long), mask)\n","        phi_t = torch.masked_select(torch.Tensor(phi_padded)\n","                                   .type(dtype=torch.float), mask)\n","        psi_t = torch.masked_select(torch.Tensor(psi_padded)\n","                                   .type(dtype=torch.float), mask)\n","\n","        \n","        pos = torch.masked_select(torch.Tensor(tertiary_padded), mask)\\\n","                  .view(3, -1).transpose(0, 1).unsqueeze(1)\n","\n","\n","\n","        \n","        tertiary = pos.squeeze(1)\n","\n","        evol = torch.masked_select(torch.Tensor(evo_padded), mask)\\\n","                  .view(21, -1).transpose(0, 1).unsqueeze(1) / 100\n","\n","        \n","        evolutionary = evol.squeeze(1)\n","\n","        primary_padded = np.zeros(MAX_SEQUENCE_LENGTH)\n","        tertiary_padded = np.zeros((MAX_SEQUENCE_LENGTH, 3))\n","        evo_padded = np.zeros((MAX_SEQUENCE_LENGTH,21))\n","        phi_padded = np.zeros(MAX_SEQUENCE_LENGTH)\n","        psi_padded = np.zeros(MAX_SEQUENCE_LENGTH)\n","\n","        length_after_mask_removed = len(prim)\n","\n","        primary_padded[:length_after_mask_removed] = prim.data.cpu().numpy()\n","        tertiary_padded[:length_after_mask_removed, :] = tertiary.data.cpu().numpy()\n","        evo_padded[:length_after_mask_removed, :] = evolutionary.data.cpu().numpy()\n","        phi_padded[:length_after_mask_removed] = phi_t.data.cpu().numpy()\n","        psi_padded[:length_after_mask_removed] = psi_t.data.cpu().numpy()\n","\n","        mask_padded = np.zeros(MAX_SEQUENCE_LENGTH)\n","        mask_padded[:length_after_mask_removed] = np.ones(length_after_mask_removed)\n","        \n","        dict_ = {}\n","        dict_['primary'] = primary_padded\n","        dict_['tertiary'] = tertiary_padded\n","        dict_['mask_padded'] = mask_padded\n","        dict_['evolutionary'] = evo_padded\n","        dict_['seq_len'] = sequence_length\n","        dict_['phi'] = phi_padded\n","        dict_['psi'] = psi_padded\n","        all_proteins.append(dict_)\n","\n","\n","    return all_proteins\n","\n","def sequence_onehot(seq):\n","    \"\"\"Maps the given sequence into a one-hot encoded matrix.\"\"\"\n","    one_hot = np.zeros((len(seq), 20), dtype=np.int32)\n","\n","    for aa_index, aa_id in enumerate(seq):\n","        one_hot[int(aa_index), int(aa_id) - 1] = 1\n","\n","    return torch.Tensor(one_hot).float()\n","\n","def seq2block(prim_msa_i, prim_msa_j):\n","    # takes each prot fi and returns n*n*f block were fij = |fi cat fj|\n","    # todo: add options to have dim=2 be different options such as |fi cat fj|fi * fj|\n","    # prim_msa shape should be (prot_len x 41d)\n","    seq_len = prim_msa_i.shape[0]\n","    fij = torch.zeros((seq_len, seq_len, prim_msa_i.shape[1] * 2))\n","    for idx, i in enumerate(prim_msa_i):\n","        for jdx, j in enumerate(prim_msa_j):\n","            fij[idx][jdx] = torch.cat((i, j))\n","    return fij\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zc8CvfvX_EvN","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.utils.model_zoo as model_zoo\n","from torch.nn.functional import softmax \n","import pickle as pkl\n","import numpy as np\n","import random\n","import torch.optim as optim\n","import subprocess\n","import torch.utils.data\n","#helper functions\n","#GLOBAL VARIABLE\n","INPUT_DIM = 82\n","OUTPUT_BINS = 65 #number of bins in output\n","ANGLE_BINS = 36\n","RESNET_DIM = 128 #number of layers inside of resnet\n","\n","\n","class BasicBlock(nn.Module):\n","\n","    def __init__(self, dilation = 1):\n","        super(BasicBlock, self).__init__()\n","        norm_layer = nn.BatchNorm2d\n","        self.project_down = conv1x1(128, 64, stride=1)\n","        self.project_up   = conv1x1(64, 128, stride=1)\n","        self.bn64_1 = norm_layer(64)\n","        self.bn64_2 = norm_layer(64)\n","        self.bn128 = norm_layer(128)\n","\n","        #dilations deal now with 64 incoming and 64 outcoming layers\n","        self.dilation = conv3x3(64, 64, stride=1, dilation = dilation) #when the block is initialized, the only thing that changes is the dilation filter used!\n","        self.elu = nn.ELU(inplace=True)\n","\n","    def forward(self, x):\n","        \n","        identity = x\n","    \n","        #the deepmind basic block goes:\n","        \n","        #batchnorm\n","        out = self.bn128(x)\n","        \n","        #elu\n","        out = self.elu(out)\n","    \n","        #project down to 64\n","        out = self.project_down(out)\n","        \n","        #batchnorm\n","        out = self.bn64_1(out)\n","\n","        #elu\n","        out = self.elu(out)   \n","        \n","        #cycle through 4 dilations\n","        out = self.dilation(out)  \n","        \n","        #batchnorm\n","        out = self.bn64_2(out)\n","\n","        #elu\n","        out = self.elu(out)\n","        \n","        #project up to 128\n","        out = self.project_up(out)\n","        \n","        #identitiy addition \n","        out = out + identity\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","\n","    def __init__(self,n_groups):\n","        super(ResNet, self).__init__()\n","        self.inplanes = RESNET_DIM\n","        \n","        self.conv1 = conv1x1(INPUT_DIM, RESNET_DIM, stride=1)\n","        self.conv2 = conv1x1(RESNET_DIM, OUTPUT_BINS )\n","        \n","        self.phi_i_conv  =  conv1x64(RESNET_DIM,ANGLE_BINS)\n","        self.phi_j_conv  =  conv1x64(RESNET_DIM,ANGLE_BINS)\n","        self.psi_i_conv  =  conv1x64(RESNET_DIM,ANGLE_BINS)\n","        self.psi_j_conv  =  conv1x64(RESNET_DIM,ANGLE_BINS)\n","\n","        self.resnet_blocks = self._make_layer(n_groups)\n","\n","\n","    def _make_layer(self, n_groups):\n","        layers = []\n","        #here I need to pass in the correct dilations 1,2,4,8\n","        dilations = [1,2,4,8]\n","        for i,_ in enumerate(range(0, n_groups)):\n","            layers.append(BasicBlock( dilation = dilations[i]))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        #fix input dimensions\n","        x = self.conv1(x)\n","  \n","        #propagate through RESNET blocks\n","        resnet_out = self.resnet_blocks(x)\n","        #renet_out has shape 1,128,64,64\n","        phii_out = self.phi_i_conv(resnet_out)\n","        phij_out = self.phi_j_conv(resnet_out)\n","        psii_out = self.psi_i_conv(resnet_out)\n","        psij_out = self.psi_j_conv(resnet_out)\n","\n","        #fix output dimensions\n","        x = self.conv2(resnet_out)\n","        #FIX THIS TO WORK WITH BATCHES!\n","        m = nn.Softmax2d()\n","        return (m(x),m(phii_out),m(phij_out),m(psii_out),m(psij_out))\n","\n","\n","def is_training():\n","    pass  # change BATCH_SIZE\n","\n","def conv3x3(in_planes, out_planes, stride=1, dilation = 1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    padding = 1 + (dilation -1 ) #derived to ensure consistent size\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=padding, bias=True, dilation = dilation)\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=True)\n","def conv1x64(in_planes, out_planes, stride=1, groups=1):\n","    \"\"\"64x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=(1,64), stride=stride, groups=groups, bias=True)\n","def conv64x1(in_planes, out_planes, stride=1, groups=1):\n","    \"\"\"64x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=(64,1), stride=stride, groups=groups, bias=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kWE8laFTpqV0","colab_type":"code","colab":{}},"source":["from torch.utils.data import IterableDataset\n","class Sequences(IterableDataset):\n","  def __init__(self, final_input):\n","    self.data = final_input # [(prim_msa,tertiary,binnedphi,binnedpsi,seqlen),(),....,()]\n","    self.bins= np.arange(float(2),float(22), float(20/64))\n","  def get_tile(self, p, i , j):\n","    prim_msa_i = self.data[p][0][i:i+64]\n","    prim_msa_j = self.data[p][0][j:j+64]\n","    return seq2block(prim_msa_i,prim_msa_j)\n","  def get_dmat(self, p, i, j):\n","    prim_msa_i = self.data[p][1][i:i+64]\n","    prim_msa_j = self.data[p][1][j:j+64]\n","    return calc_pairwise_distances(prim_msa_i,prim_msa_j,False)\n","  def bin_dmat(self, dmat):\n","    return np.digitize(dmat,self.bins)\n","  def __len__(self):\n","    return len(self.data)\n","  def __iter__(self):\n","    self.p =0\n","    self.i=0\n","    self.j=0\n","    return self\n","  def __next__(self):\n","    if(self.p<len(self.data)):\n","      a = self.get_tile(self.p,self.i,self.j)\n","    else:\n","      raise StopIteration\n","    a = a.permute(2,0,1)\n","    b_unbinned = self.get_dmat(self.p,self.i,self.j)\n","    b = self.bin_dmat(b_unbinned)\n","\n","    phi_i = self.data[self.p][2][self.i:self.i+64].unsqueeze(1)\n","    phi_j = self.data[self.p][2][self.j:self.j+64].unsqueeze(1)\n","    psi_i = self.data[self.p][3][self.i:self.i+64].unsqueeze(1)\n","    psi_j = self.data[self.p][3][self.j:self.j+64].unsqueeze(1)\n","\n","    ret_i = self.i\n","    ret_j = self.j\n","    ret_n = self.data[self.p][4]\n","    ret_p = self.p\n","    self.j = self.j+32\n","    if( self.j > len(self.data[self.p][0])-64):\n","      self.j = 0\n","      self.i += 32\n","    if(self.i > len(self.data[self.p][0])-64 ):\n","      self.i =0\n","      self.j =0\n","      self.p +=1\n","    if(self.p>=len(self.data)):\n","        raise StopIteration\n","    return (a,b,(phi_i,phi_j,psi_i,psi_j),(ret_i,ret_j, ret_n, ret_p))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nbes7wsS-cXm","colab_type":"code","colab":{}},"source":["train = process_file(training_file,use_gpu=False)\n","\n","valid = process_file(validation_file,use_gpu=False)\n","\n","test = process_file(testing_file,use_gpu=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zaEeSZWhs4w_","colab_type":"code","colab":{}},"source":["# one hotting the primary and concatenating with MSA\n","for protein in train:\n","  one_hot = sequence_onehot(protein['primary'])\n","  evo_tensor = torch.Tensor(protein['evolutionary']).float()\n","  protein['prim_msa_cat'] = torch.cat((one_hot,evo_tensor),1)\n","\n","# one hotting the primary and concatenating with MSA -- validation data\n","for protein in valid:\n","  one_hot = sequence_onehot(protein['primary'])\n","  evo_tensor = torch.Tensor(protein['evolutionary']).float()\n","  protein['prim_msa_cat'] = torch.cat((one_hot,evo_tensor),1)\n","\n","# one hotting the primary and concatenating with MSA -- validation data\n","for protein in test:\n","  one_hot = sequence_onehot(protein['primary'])\n","  evo_tensor = torch.Tensor(protein['evolutionary']).float()\n","  protein['prim_msa_cat'] = torch.cat((one_hot,evo_tensor),1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-2kdS8F4l9R5","colab_type":"code","colab":{}},"source":["#bin phi/psi values\n","import math\n","for protein in train:\n","  bins= np.arange(float(-math.pi),float(math.pi), float((2*math.pi)/35))\n","  protein['phi_binned'] = np.digitize(protein['phi'],bins)\n","  protein['psi_binned'] = np.digitize(protein['psi'],bins)\n","\n","for protein in valid:\n","  bins= np.arange(float(-math.pi),float(math.pi), float((2*math.pi)/35))\n","  protein['phi_binned'] = np.digitize(protein['phi'],bins)\n","  protein['psi_binned'] = np.digitize(protein['psi'],bins)\n","\n","for protein in test:\n","  bins= np.arange(float(-math.pi),float(math.pi), float((2*math.pi)/35))\n","  protein['phi_binned'] = np.digitize(protein['phi'],bins)\n","  protein['psi_binned'] = np.digitize(protein['psi'],bins)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RzNMaycvKG4j","colab_type":"code","colab":{}},"source":["#staging input features, labels and seq len for Dataloader\n","final_input = [] #[(primmsa,tertiary,phibin,psibin,seqlen)]\n","for protein in train:\n","  final_input.append((protein['prim_msa_cat'],torch.Tensor(protein['tertiary']).float(),\n","                      torch.Tensor(protein['phi_binned']).long(),\n","                      torch.Tensor(protein['psi_binned']).long(),\n","                      protein['seq_len']))\n","\n","valid_input = []\n","for protein in valid:\n","  valid_input.append((protein['prim_msa_cat'],torch.Tensor(protein['tertiary']).float(),\n","                      torch.Tensor(protein['phi_binned']).long(),\n","                      torch.Tensor(protein['psi_binned']).long(),\n","                      protein['seq_len']))\n","\n","test_input = []\n","for protein in test:\n","  test_input.append((protein['prim_msa_cat'],torch.Tensor(protein['tertiary']).float(),\n","                     torch.Tensor(protein['phi_binned']).long(),\n","                     torch.Tensor(protein['psi_binned']).long(),\n","                     protein['seq_len']))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6DhLdvpAKGAO","colab_type":"code","outputId":"8300e195-c4e6-48be-ee64-55264c701557","executionInfo":{"status":"ok","timestamp":1588525207404,"user_tz":240,"elapsed":746,"user":{"displayName":"Rahul Puppala","photoUrl":"https://lh6.googleusercontent.com/-a3E4o0T7KT8/AAAAAAAAAAI/AAAAAAAAAq4/SoPeLsACGtc/s64/photo.jpg","userId":"12827901970476877756"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["print(len(final_input))\n","print(len(valid_input))\n","print(len(test_input))\n","print(final_input[0][2].shape)\n","print(final_input[0][3].shape)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["11\n","11\n","11\n","torch.Size([100])\n","torch.Size([100])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WZScCyA1z_hP","colab_type":"code","colab":{}},"source":["train_data = Sequences(final_input)\n","dataloader = torch.utils.data.DataLoader(train_data, batch_size =1, num_workers = 0)\n","\n","valid_data = Sequences(valid_input)\n","valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size =4, num_workers = 1)\n","\n","test_data = Sequences(test_input)\n","test_dataloader = torch.utils.data.DataLoader(test_data, batch_size =1, num_workers = 1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoMs2n2_0WLd","colab_type":"code","colab":{}},"source":["def train_model(model, n_epochs):\n","    train_losses = []\n","    # to track the validation loss as the model trains\n","    valid_losses = []\n","    # to track the average training loss per epoch as the model trains\n","    avg_train_losses = []\n","    # to track the average validation loss per epoch as the model trains\n","    avg_valid_losses = []\n","    \n","    \n","    for e in range(1,n_epochs+1):\n","        model.train()\n","        ###################\n","        # train the model #\n","        ###################\n","        #(a,b,(phi_i,phi_j,psi_i,psi_j),(ret_i,ret_j, ret_n, ret_p))\n","        for(batch,(data,target,phipsi,(_,_,_,_))) in enumerate(dataloader):\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output[0], target)\n","            loss += criterion(output[1],phipsi[0]) #phi i\n","            loss += criterion(output[2],phipsi[1]) #phi j\n","            loss += criterion(output[3],phipsi[2]) #psi i\n","            loss += criterion(output[4],phipsi[3]) #psi j\n","            loss.backward()\n","            optimizer.step()\n","            train_losses.append(loss.item())\n","        ######################    \n","        # validate the model #\n","        ######################\n","        model.eval() # prep model for evaluation\n","  \n","        for(batch,(data, target,phipsi,(_,_,_,_))) in enumerate(valid_dataloader):\n","            print(batch)\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = model(data)\n","            # calculate the loss\n","            loss = criterion(output[0], target)\n","            loss += criterion(output[1],phipsi[0]) #phi i\n","            loss += criterion(output[2],phipsi[1]) #phi j\n","            loss += criterion(output[3],phipsi[2]) #psi i\n","            loss += criterion(output[4],phipsi[3]) #psi j\n","            # record validation loss\n","            valid_losses.append(loss.item())\n","        train_loss = np.average(train_losses)\n","        valid_loss = np.average(valid_losses)\n","        avg_train_losses.append(train_loss)\n","        avg_valid_losses.append(valid_loss)\n","        epoch_len = len(str(n_epochs))\n","        \n","        print_msg = (f'[{e:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n","                      f'train_loss: {train_loss:.5f} ' +\n","                      f'valid_loss: {valid_loss:.5f}')\n","\n","        print(print_msg)\n","          \n","        # clear lists to track next epoch\n","        train_losses = []\n","        valid_losses = []\n","    return  model, avg_train_losses, avg_valid_losses"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTr4zkLF0d7g","colab_type":"code","colab":{}},"source":["model = ResNet(ng_val)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters())\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eaT8E94TIj3Q","colab_type":"code","outputId":"dab48acc-cf1d-473f-bfa2-39f9e6b2722a","executionInfo":{"status":"ok","timestamp":1588525974509,"user_tz":240,"elapsed":13936,"user":{"displayName":"Rahul Puppala","photoUrl":"https://lh6.googleusercontent.com/-a3E4o0T7KT8/AAAAAAAAAAI/AAAAAAAAAq4/SoPeLsACGtc/s64/photo.jpg","userId":"12827901970476877756"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"source":["model, train_losses, valid_losses = train_model(model,1)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","[1/1] train_loss: 17.21141 valid_loss: 16.50090\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QZwfLhj4d3D4","colab_type":"code","colab":{}},"source":["import math\n","def Average(lst): \n","    return sum(lst) / len(lst) \n","def test_model(model):\n","  test_dataloader = torch.utils.data.DataLoader(test_data, batch_size =1, num_workers = 1)\n","  all_preds = []\n","  p_len = {}\n","  for (batch,(data,target,phipsi,(i_off,j_off,seq_len,p_num))) in enumerate(test_dataloader):\n","    p_len[p_num.item()] = seq_len.item()\n","  for i in range(0,len(test_data)):\n","    temp = [[1 for j in range(MAX_SEQUENCE_LENGTH)] for i in range(MAX_SEQUENCE_LENGTH)]\n","    for j in range(0,MAX_SEQUENCE_LENGTH):\n","      for k in range(0,MAX_SEQUENCE_LENGTH):\n","        temp[j][k] = (-1,0,0) # (true label , summed probability, contact or not)\n","    all_preds.append(temp)\n","  for (batch,(data,target,phipsi,(i_off,j_off,seq_len,p_num))) in enumerate(test_dataloader):\n","    output = model(data)\n","    output = output[0]\n","    output = output.squeeze(0)\n","    target = target.squeeze(0)\n","    summed = torch.sum(output[0:20],dim=0)  \n","    for i in range(0,64):\n","      for j in range(0,64):\n","        if(all_preds[p_num][i+i_off][j+j_off][0] == -1):\n","          if(target[i][j]<20):\n","            x=1\n","          else:\n","            x=0\n","          y = summed[i][j].item()\n","          if(all_preds[p_num][i+i_off][j+j_off][1]>=.5):\n","            z = 1\n","          else:\n","            z = 0\n","          all_preds[p_num][i+i_off][j+j_off] = (x,y,z)\n","\n","  accuracy = []\n","  accuracy2 = []\n","  accuracy5 = []\n","  for i,preds in enumerate(all_preds):\n","    num_correct = 0\n","    total = 0\n","    for j in range(0,min(95,p_len[i])):\n","      for k in range(0,min(95,p_len[i])):\n","        total+=1\n","        if(preds[j][k][0]==preds[j][k][2]):\n","          num_correct+=1\n","    accuracy.append(num_correct/total)\n","    flat = [j for sub in preds[0:p_len[i]][0:p_len[i]] for j in sub] \n","    flat.sort(key=lambda x:x[1])\n","    flat_2 = flat[0:math.floor(len(flat)/2)]\n","    flat_5 = flat[0:math.floor(len(flat)/5)]\n","    num_correct=0\n","    total=0\n","    for j in range(0,len(flat_2)):\n","      total+=1\n","      if(flat_2[j][0]==flat_2[j][2]):\n","        num_correct+=1\n","    accuracy2.append(num_correct/total)\n","    num_correct=0\n","    total=0\n","    for j in range(0,len(flat_5)):\n","      total+=1\n","      if(flat_5[j][0]==flat_5[j][2]):\n","        num_correct+=1\n","    accuracy5.append(num_correct/total)\n","  print(\"N-Accuracy: \",Average(accuracy))\n","  print(\"N/2-Accuracy: \",Average(accuracy2))\n","  print(\"N/5-Accuracy: \",Average(accuracy5))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NiiGp9F1iYWb","colab_type":"text"},"source":["Initial testing is for tertiary label prediction"]},{"cell_type":"code","metadata":{"id":"1H9oblBue3da","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"15720073-2889-4f84-f6be-b90c241ea18a","executionInfo":{"status":"ok","timestamp":1588526049891,"user_tz":240,"elapsed":13071,"user":{"displayName":"Rahul Puppala","photoUrl":"https://lh6.googleusercontent.com/-a3E4o0T7KT8/AAAAAAAAAAI/AAAAAAAAAq4/SoPeLsACGtc/s64/photo.jpg","userId":"12827901970476877756"}}},"source":["test_model(model)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["N-Accuracy:  0.6110701547315175\n","N/2-Accuracy:  0.5905577225203582\n","N/5-Accuracy:  0.502278763983636\n"],"name":"stdout"}]}]}